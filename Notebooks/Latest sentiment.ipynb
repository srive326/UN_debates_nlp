{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "join() argument must be str or bytes, not 'PosixPath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3dc15d00d097>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mdebates_paragraphs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0mdebates_paragraphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/NLP/UN_debates_nlp/Notebooks/src/utils/corpus.py\u001b[0m in \u001b[0;36mload_corpus\u001b[0;34m(paragraph_tokenize)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/processed/debates.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHOME_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.5/posixpath.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mpath\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBytesWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mgenericpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_arg_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'join'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/genericpath.py\u001b[0m in \u001b[0;36m_check_arg_types\u001b[0;34m(funcname, *args)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             raise TypeError('%s() argument must be str or bytes, not %r' %\n\u001b[0;32m--> 143\u001b[0;31m                             (funcname, s.__class__.__name__)) from None\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasstr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasbytes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't mix strings and bytes in path components\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: join() argument must be str or bytes, not 'PosixPath'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from src import HOME_DIR\n",
    "from src.utils.tokenization import ParagraphTokenizer\n",
    "from src.utils.tokenization import WordTokenizer\n",
    "from src.utils.corpus import load_corpus\n",
    "\n",
    "def preprocess_data():\n",
    "    \"\"\"Load and preprocess the raw data\n",
    "\n",
    "    This helper function loads and preprocesses the data to abstract away some\n",
    "    of the menial work.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    paragraph_tokenize : bool\n",
    "        Indicate whether to paragraph tokenize the speeches.\n",
    "    \"\"\"\n",
    "    debates = pd.read_csv(\n",
    "        os.path.join('un-general-debates.csv'))\n",
    "    \n",
    "    iso_codes = pd.read_csv(\n",
    "        os.path.join('wikipedia-iso-country-codes.csv'),\n",
    "        usecols=['English short name lower case', 'Alpha-3 code'])\n",
    "    iso_codes.columns = ['country_name', 'country']\n",
    "    \n",
    "    # Certain codes were missing, so need to add manually.\n",
    "    iso_codes = iso_codes.append(\n",
    "        pd.DataFrame({\n",
    "            'country_name': ['Democratic Yemen', 'Czechoslovakia',\n",
    "                             'Yugoslavia', 'East Germany', 'European Union',\n",
    "                             'South Sudan'],\n",
    "            'country': ['YDYE', 'CSK', 'YUG', 'DDR', 'EU', 'SSD']\n",
    "        }),\n",
    "        sort=False\n",
    "    )\n",
    "    debates = pd.merge(debates, iso_codes, how='left', on='country')\n",
    "    debates = debates.sort_values(['year', 'country']).reset_index(drop=True)\n",
    "\n",
    "    paragraph_tokenizer = ParagraphTokenizer()\n",
    "    paragraphs = pd.Series(\n",
    "        debates.text\n",
    "        .apply(lambda x: [x[start:end] for start, end\n",
    "                          in paragraph_tokenizer.span_tokenize(x)])\n",
    "        .apply(lambda x: pd.Series(x))\n",
    "        .stack()\n",
    "        .reset_index(level=1, drop=True), name='text')\n",
    "    debates_paragraphs = (debates\n",
    "                          .drop('text', axis=1)\n",
    "                          .join(paragraphs)\n",
    "                          .reset_index())\n",
    "    # Must retain this new index to preserve ordering of paragraphs within\n",
    "    # each speech.\n",
    "    debates_paragraphs.index.name = 'paragraph_index'\n",
    "\n",
    "    # Save data to interim directory.\n",
    "    debates.to_csv(\n",
    "        os.path.join('data/debates.csv'),\n",
    "        index=False)\n",
    "    debates_paragraphs.to_csv(\n",
    "        os.path.join('data/debates_paragraphs.csv'),\n",
    "        index=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    preprocess_data()\n",
    "\n",
    "debates_paragraphs = load_corpus()\n",
    "debates_paragraphs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
